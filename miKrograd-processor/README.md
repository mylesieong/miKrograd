# miKrograd KSP Processor

This module contains the Kotlin Symbol Processing (KSP) processor for the miKrograd library. The processor generates optimized code for functions annotated with the `@Mikrograd` annotation.

## Features

- **Compile-Time Mode Selection**: Choose between inference-only (ForwardPassNode) and training (BackpropNode) modes at compile time.
- **Optimized Code Generation**: Generate optimized code for mathematical expressions based on the selected mode.
- **Direct Node Generation**: Generates code that directly instantiates ForwardPassNode or BackpropNode based on computation mode.

## Usage

### Basic Usage

```kotlin
// Inference-only mode (default)
@Mikrograd
fun inferenceExpression() {
    3.0 * 4.0 + (7.0 + 3.0)
}

// Training mode with gradient tracking
@Mikrograd(mode = ComputationMode.TRAINING)
fun trainingExpression() {
    3.0 * 4.0 + (7.0 + 3.0)
}
```

### Computation Modes

The `@Mikrograd` annotation supports two computation modes:

1. **INFERENCE** (default): Uses `ForwardPassNode` which doesn't track gradients. This is more memory-efficient when only forward pass is needed.
2. **TRAINING**: Uses `BackpropNode` which tracks gradients for backpropagation. This is necessary when gradient computation is needed.

## Implementation Details

### Annotation

The `@Mikrograd` annotation is defined in the `miKrograd-annotations` module:

```kotlin
enum class ComputationMode {
    INFERENCE,
    TRAINING
}

@Target(AnnotationTarget.FUNCTION)
@Retention(AnnotationRetention.SOURCE)
annotation class Mikrograd(val mode: ComputationMode = ComputationMode.INFERENCE)
```

### Processor

The `ComputeGraphProcessor` processes functions annotated with `@Mikrograd`:

1. Extracts the computation mode from the annotation
2. Parses the function body to build an abstract syntax tree (AST)
3. Uses a visitor pattern to generate code based on the computation mode
4. Writes the generated code to a new file

### Visitors

The processor uses different visitors to generate code based on the computation mode:

- `DifferentiationVisitor`: Generates code that uses either `ForwardPassNode` or `BackpropNode` based on the computation mode.
- `CodeGeneratingVisitor`: The original visitor that generates code using the `ComputeNode` classes.

## Benefits of KSP-Based Approach

1. **Compile-Time Safety**: Mode selection errors are caught at compile time.
2. **Performance**: No runtime overhead for mode selection.
3. **Optimized Code**: Generated code is optimized for the selected mode.
4. **Clear Intent**: The mode is explicitly specified in the annotation.

## Example Generated Code

For a function annotated with `@Mikrograd`:

```kotlin
@Mikrograd(ComputationMode.INFERENCE)
fun testExpr() {
    3.0 * 5.0 + (7.0 + 3.0)
}
```

The processor generates:

```kotlin
// Generated by ComputeGraphProcessor - Mode: INFERENCE
public fun testExprGenerated(): AutoDiffNode {
  val value0 = ForwardPassNode(3.0)
  val value1 = ForwardPassNode(5.0)
  val multiply2 = value0 * value1
  val value3 = ForwardPassNode(7.0)
  val value4 = ForwardPassNode(3.0)
  val add5 = value3 + value4
  val add6 = multiply2 + add5
  return add6
}
```

For a function annotated with `@Mikrograd(mode = ComputationMode.TRAINING)`, the generated code uses `BackpropNode` instead of `ForwardPassNode` and returns `BackpropNode` instead of `AutoDiffNode`.
